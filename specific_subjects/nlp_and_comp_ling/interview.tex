\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[pagebackref=true]{hyperref}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{bbm}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{xcolor}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}\hspace{-2.5pt}}

\newcommand{\followup}[1]{\textcolor{red}{ #1 }}
\newcommand{\Ubr}[2]{\underbrace{ #1 }_{\mathclap{\text{ #2 }}}}
\newcommand{\Br}[1]{\{ #1 \}}

\geometry{margin=2cm}

\title{Interview Preparation: NLP / Deep Learning}
\author{Isaac Riley}
\date{Last edited: \today}

\begin{document}
\maketitle
\tableofcontents
\newpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{section}{-1}
\section{Allgemeines}

%========================================================
\subsection{DeepAI Definitions}
\begin{itemize}
    \item [\done] Accuracy in Machine Learning
    \item [\done] Activation Function \\ 
    \begin{itemize}
        \item $\frac{dS(x)}{dx} = S(x)(1-S(x))$
        \item PReLU: identity at 0 and above; multiplied by a constant below 0
    \end{itemize}
    
    \item [\done] Activation Level: output generated by an activation function
    \item [\done]  Active Learning: algorithm chooses which data to learn from; many query strategies (\followup{least confidence, margin sampling, entropy sampling})
    \begin{itemize}
        \item \followup{membership query synthesis}
        \item \followup{stream-based selective sampling}
        \item \followup{pool-based sampling}
    \end{itemize}
    \item [\done] Adam Optimizer: optimization algorithm combing AdaGrad and RMSProp; adapts parameter learning rates in real-time based on exponential moving the gradient and the squared gradient; $\beta_1$ and $\beta_2$ control decay rates; also includes \followup{bias correction estimates} before updating the learning parameters
    \item [\done] AdaGrad (Adaptive Subgradient Methods) - variation of SGD that updates the learning rate for each parameter; soeeds up training especially well for sparse datasets; problem: accumulation of squared gradients in the denominator
    \item [\done] Admissible Decision Rule: function for deciding which available course of action will generate the best result; in frequentist approaches, the rule with the best balance between loss function (cost) and expectation function (risk); in Bayesian approaches, all in terms of probabilities; essence: the rule such that no alternative rule is better
    \item [\done] Adversarial Machine Learning: techniques for training neural networks to spot intentionally misleading data or behaviors, which typically fall into the categories of classification evasion or data poisoning; two main techniques are adversiarial training and \followup{defensive distillation} - AML is security-oriented and distinct from GANs
    \item [\done] Affine Layer: fully connected layer (with bias)
    \item [\done] Affinity Matrix: matix of mutual similarity between a set of data points; implicit in unsupervised learning
    \item [\done] Anomaly Detection: family of approaches used to find outliers, often for error/fraud detection
    \item [\done] Association Learning
    \item [\done] Associations: the specific measurable constraints on interestingness used in association rule learning - most commonly used:
    \begin{itemize}
        \item \followup{support}: how frequently the pattern occurs in the dataset
        \item \followup{confidence}: how often the rule being used has been true (conditional probability)
        \item \followup{lift}: actual success rate relative to random chance
        \item conviction: actual incorrect prediction rate over the expected failure rate from random chance
    \end{itemize}
    \item [\done] Asymptotic Analysis
    \begin{itemize}
        \item $O(n)$: worst-case run time
        \item $\Omega(n)$: best-case run time
        \item $\Theta(n)$: average run time
    \end{itemize}
    \item [\done] \followup{Attention Models}: attention is a function that maps a query and an ``s set'' of key-value pairs to an output; weights for the values are determined by a compatibility function
    \item [\done] Autoencoders: types: denoising, sparse, \followup{variational}, \followup{contractive}
    \item [\done] Automated Reasoning
    \item [\done] Automatic Speech Recognition
    \item [\done] Autoregressive Model
    \item [\done] Backpropagation
    \item [\done] Batch Normalization
    \item [\done] Bayes Factor: ratio between likelihoods of competing hypotheses; seeks to quantify support levels for each hypothesis without a hard decision boundary 
    \item [\done] Bayes' Theorem
    \item [\done] \followup{Bayesian Efficiency}: similar to Pareto efficiency, but with within a Bayesian framework
    \item [\done] Bayesian Inference
    \item [\done] Bayesian Networks
    \item [\done] Bayesian Probability
    \item [\done] Bayesian Programming: an approach involving construction of probability models to solve open-ended roblems with incomplete information
    \item [\done] Bayesian Statistics: probability as ``degrees of belief''; consists largely of inference, modeling, and experimental design; good for datasets with few data points for reference, models with strong prior intuitions from previous observations, data with high levels of uncertainty, comparison between models, settings where it is useful to claim something about the likelihood of an alternative hypothesis (alternative to the model's null hypothesis)
    \item [\done] Bernoulli Distribution: binary distribution; lays the groundwork for \followup{binomial distribution, geometric distribution, negative binomial distribution}
    \item [\done] Beta Distribution: statistical distribution of a probability value itself
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    \item [\done] 
    
\end{itemize}
%--------------------------------------------------
\subsubsection{}


\section{ML Interviews Book}
%--------------------------------------------------
\subsubsection{}
\begin{enumerate}
    \item Why do we need dimensionality reduction?

    \item Eignendecomposition is a common factorization technique used for dimensionality reduction. Is the eigendecomposition of a matrix always unique?

    \item Name some applications of eigenvalues and eigenvectors.
    
    \item We want to do PCA on  dataset of multiple features in different ranges. For example, one is in the range 0-1 and one is in the range 10-1000. Will PCA work on this dataset?
    
    \item 


\end{enumerate}

%--------------------------------------------------
\subsubsection{}

%--------------------------------------------------
\subsubsection{}

%--------------------------------------------------
\subsubsection{}

%--------------------------------------------------
\subsubsection{}



\section{Mathematische FÃ¤higkeiten}

%========================================================
\subsection{}

%--------------------------------------------------
\subsubsection{}






\end{document}